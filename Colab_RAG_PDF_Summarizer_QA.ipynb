{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40d25c57",
   "metadata": {},
   "source": [
    "\n",
    "# üìö Custom RAG for PDFs ‚Äî Summarization + QA Chatbot (Open‚ÄëSource LLM)\n",
    "*Colab-ready notebook generated on 2025-10-29 08:36.*\n",
    "\n",
    "**What you get:**\n",
    "- PDF ingestion & chunking\n",
    "- Chroma vector store with `sentence-transformers/all-MiniLM-L6-v2`\n",
    "- Optional reranking with `BGE` (`bge-reranker-base` via `FlagEmbedding`)\n",
    "- Open‚Äësource LLM (choose one):\n",
    "  - `mistralai/Mistral-7B-Instruct-v0.2` *(default)*\n",
    "  - `NousResearch/Meta-Llama-3.1-8B-Instruct` *(works on Colab with 4‚Äëbit)*\n",
    "- Map‚ÄëReduce summarization of full corpus\n",
    "- RAG QA chain (retrieval ‚Üí rerank ‚Üí grounded answer + citations)\n",
    "- Simple **Gradio** chatbot UI\n",
    "\n",
    "> Tip: Start with Mistral 7B in 4‚Äëbit (fast & light), then try Llama 3.1 8B if you have more VRAM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be298f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys, platform, torch\n",
    "print(\"Python:\", sys.version)\n",
    "print(\"Platform:\", platform.platform())\n",
    "print(\"Torch:\", torch.__version__ if torch.cuda.is_available() else \"not available\")\n",
    "print(\"CUDA available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82489bc6",
   "metadata": {},
   "source": [
    "## 1) Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0d7a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "%%capture\n",
    "!pip -q install --upgrade pip\n",
    "!pip -q install langchain==0.3.7 langchain-community==0.3.7 langchain-text-splitters==0.3.2\n",
    "!pip -q install chromadb==0.5.12 sentence-transformers==3.2.1\n",
    "!pip -q install pypdf==5.0.1 pymupdf==1.24.10\n",
    "!pip -q install transformers==4.46.1 accelerate==0.34.2 bitsandbytes==0.44.1\n",
    "!pip -q install FlagEmbedding==1.2.11\n",
    "!pip -q install gradio==4.44.0\n",
    "!pip -q install pydantic==2.9.2 pydantic-settings==2.5.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f33577f",
   "metadata": {},
   "source": [
    "## 2) Configure models & paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb3534",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass\n",
    "class RAGConfig:\n",
    "    # LLM options: 'mistral' or 'llama31'\n",
    "    llm_choice: str = \"mistral\"  # \"mistral\" | \"llama31\"\n",
    "    mistral_repo: str = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "    llama_repo: str = \"NousResearch/Meta-Llama-3.1-8B-Instruct\"\n",
    "    load_4bit: bool = True\n",
    "    max_new_tokens: int = 512\n",
    "    temperature: float = 0.2\n",
    "    top_p: float = 0.9\n",
    "\n",
    "    embedding_model: str = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    use_bge_reranker: bool = True\n",
    "    bge_reranker: str = \"BAAI/bge-reranker-base\"\n",
    "\n",
    "    persist_dir: str = \"chroma_store\"\n",
    "    collection_name: str = \"pdf_rag\"\n",
    "    chunk_size: int = 1000\n",
    "    chunk_overlap: int = 200\n",
    "\n",
    "cfg = RAGConfig()\n",
    "print(cfg)\n",
    "Path(cfg.persist_dir).mkdir(exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9431039d",
   "metadata": {},
   "source": [
    "## 3) Upload PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf280ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from google.colab import files\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "pdf_dir = Path(\"pdfs\")\n",
    "pdf_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"‚û°Ô∏è Choose your PDF ebooks or documents to ingest...\")\n",
    "uploaded = files.upload()  # opens file picker\n",
    "for fname, _ in uploaded.items():\n",
    "    shutil.move(fname, pdf_dir / fname)\n",
    "\n",
    "list(pdf_dir.glob(\"*.pdf\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "795a9aac",
   "metadata": {},
   "source": [
    "## 4) Parse PDFs ‚Üí text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dc3bf59",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.schema import Document\n",
    "\n",
    "docs = []\n",
    "for pdf_path in sorted(pdf_dir.glob(\"*.pdf\")):\n",
    "    try:\n",
    "        loader = PyPDFLoader(str(pdf_path))\n",
    "        pages = loader.load()\n",
    "        docs.extend(pages)\n",
    "        print(f\"Loaded {pdf_path.name}: {len(pages)} pages\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {pdf_path.name}: {e}\")\n",
    "\n",
    "print(\"Total pages:\", len(docs))\n",
    "docs[:2]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aad642d",
   "metadata": {},
   "source": [
    "## 5) Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b819fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=cfg.chunk_size,\n",
    "    chunk_overlap=cfg.chunk_overlap,\n",
    "    separators=[ \"\\n\\n\", \"\\n\", \". \", \"? \", \"! \", \"; \", \" \", \"\"],\n",
    ")\n",
    "\n",
    "chunked_docs = splitter.split_documents(docs)\n",
    "print(\"Chunks:\", len(chunked_docs))\n",
    "chunked_docs[:1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae90942a",
   "metadata": {},
   "source": [
    "## 6) Build / Load Chroma Vector Store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97282241",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "emb = HuggingFaceEmbeddings(model_name=cfg.embedding_model)\n",
    "\n",
    "vectorstore = Chroma(\n",
    "    collection_name=cfg.collection_name,\n",
    "    persist_directory=cfg.persist_dir,\n",
    "    embedding_function=emb,\n",
    ")\n",
    "# Add only if empty (avoid duplicates on reruns)\n",
    "if vectorstore._collection.count() == 0:\n",
    "    vectorstore.add_documents(chunked_docs)\n",
    "    vectorstore.persist()\n",
    "    print(\"‚úÖ Added and persisted documents to Chroma.\")\n",
    "else:\n",
    "    print(\"‚ÑπÔ∏è Using existing Chroma collection; skipping add.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a70a5d15",
   "metadata": {},
   "source": [
    "## 7) (Optional) Reranker ‚Äî BGE base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66c66232",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "reranker = None\n",
    "if cfg.use_bge_reranker:\n",
    "    try:\n",
    "        from FlagEmbedding import FlagReranker\n",
    "        reranker = FlagReranker(cfg.bge_reranker, use_fp16=True)\n",
    "        print(\"‚úÖ BGE reranker loaded.\")\n",
    "    except Exception as e:\n",
    "        print(\"‚ö†Ô∏è Could not load BGE reranker:\", e)\n",
    "        reranker = None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671d9e7d",
   "metadata": {},
   "source": [
    "## 8) Load an open‚Äësource LLM (4‚Äëbit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b5f412d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig\n",
    "\n",
    "def load_llm(cfg):\n",
    "    if cfg.llm_choice == \"llama31\":\n",
    "        repo = cfg.llama_repo\n",
    "    else:\n",
    "        repo = cfg.mistral_repo\n",
    "\n",
    "    bnb_config = None\n",
    "    if cfg.load_4bit:\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_use_double_quant=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "        )\n",
    "\n",
    "    tok = AutoTokenizer.from_pretrained(repo, use_fast=True)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        repo,\n",
    "        device_map=\"auto\",\n",
    "        quantization_config=bnb_config,\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n",
    "    )\n",
    "\n",
    "    pipe = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tok,\n",
    "        max_new_tokens=cfg.max_new_tokens,\n",
    "        temperature=cfg.temperature,\n",
    "        top_p=cfg.top_p,\n",
    "        do_sample=True,\n",
    "        pad_token_id=tok.eos_token_id,\n",
    "    )\n",
    "    return pipe\n",
    "\n",
    "llm_pipe = load_llm(cfg)\n",
    "print(\"‚úÖ LLM pipeline ready.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "147dd4dc",
   "metadata": {},
   "source": [
    "## 9) RAG helpers (retrieve ‚Üí rerank ‚Üí answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59a09b52",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import List, Dict, Any\n",
    "import textwrap\n",
    "\n",
    "def retrieve(query: str, k: int = 6):\n",
    "    retriever = vectorstore.as_retriever(search_kwargs={\"k\": k})\n",
    "    docs = retriever.get_relevant_documents(query)\n",
    "    if reranker is not None:\n",
    "        pairs = [[query, d.page_content] for d in docs]\n",
    "        scores = reranker.compute_score(pairs)\n",
    "        scored = sorted(zip(docs, scores), key=lambda x: x[1], reverse=True)\n",
    "        docs = [d for d, s in scored]\n",
    "    return docs\n",
    "\n",
    "def make_context(docs: List, max_chars: int = 4000):\n",
    "    ctx = \"\"\n",
    "    sources = []\n",
    "    for i, d in enumerate(docs):\n",
    "        chunk = d.page_content.strip()\n",
    "        meta = d.metadata.copy()\n",
    "        src = f\"{meta.get('source', 'unknown')}#p{meta.get('page', 'NA')}\"\n",
    "        sources.append(src)\n",
    "        if len(ctx) + len(chunk) + 100 < max_chars:\n",
    "            ctx += f\"\\n[Source {i+1}: {src}]\\n{chunk}\\n\"\n",
    "        else:\n",
    "            break\n",
    "    return ctx.strip(), sources\n",
    "\n",
    "def chat_llm(prompt: str) -> str:\n",
    "    out = llm_pipe(prompt)[0][\"generated_text\"]\n",
    "    # the pipeline returns input + completion, so trim if needed\n",
    "    if out.startswith(prompt):\n",
    "        out = out[len(prompt):]\n",
    "    return out.strip()\n",
    "\n",
    "SYSTEM_QA = \"\"\"You are a precise assistant answering questions grounded ONLY in the provided context.\n",
    "If the answer cannot be found in the context, say you don't know.\n",
    "Cite sources like [1], [2] corresponding to the context chunks used.\n",
    "\"\"\"\n",
    "\n",
    "QA_PROMPT_TMPL = \"\"\"{system}\n",
    "\n",
    "Question: {q}\n",
    "\n",
    "Context:\n",
    "{ctx}\n",
    "\n",
    "Answer (with citations):\"\"\"\n",
    "\n",
    "def answer_question(q: str, k: int = 6) -> Dict[str, Any]:\n",
    "    docs = retrieve(q, k=k)\n",
    "    ctx, sources = make_context(docs)\n",
    "    prompt = QA_PROMPT_TMPL.format(system=SYSTEM_QA, q=q, ctx=ctx)\n",
    "    ans = chat_llm(prompt)\n",
    "    return {\"answer\": ans, \"sources\": sources, \"used_k\": len(docs)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e3d3ab",
   "metadata": {},
   "source": [
    "## 10) Map‚ÄëReduce corpus summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90193620",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import math\n",
    "\n",
    "MAP_PROMPT = \"\"\"You are helping summarize academic/technical texts.\n",
    "Write a concise bullet summary (3-6 bullets) of the following passage.\n",
    "Be faithful to the text; no external info.\n",
    "\n",
    "PASSAGE:\n",
    "{passage}\n",
    "\"\"\"\n",
    "\n",
    "REDUCE_PROMPT = \"\"\"You will merge multiple bullet lists into a single high-quality summary.\n",
    "- Keep it concise (6-10 bullets).\n",
    "- Remove redundancy and boilerplate.\n",
    "- Preserve key definitions, formulas, and results.\n",
    "- Add section titles if helpful.\n",
    "\n",
    "BULLET LISTS:\n",
    "{bullets}\n",
    "\"\"\"\n",
    "\n",
    "def map_reduce_summary(batch_size: int = 8, max_docs: int = 128):\n",
    "    subset = chunked_docs[:max_docs]\n",
    "    bullets = []\n",
    "    for i in range(0, len(subset), batch_size):\n",
    "        batch = subset[i:i+batch_size]\n",
    "        for d in batch:\n",
    "            prompt = MAP_PROMPT.format(passage=d.page_content[:3000])\n",
    "            bullets.append(chat_llm(prompt))\n",
    "    merged = \"\\n\\n\".join(bullets)\n",
    "    final = chat_llm(REDUCE_PROMPT.format(bullets=merged[:12000]))\n",
    "    return final\n",
    "\n",
    "# Example (optional run):\n",
    "# summary = map_reduce_summary(batch_size=6, max_docs=60)\n",
    "# print(summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19835497",
   "metadata": {},
   "source": [
    "## 11) Quick test ‚Äî ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0347792",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "q = \"What is the central thesis of the first uploaded book?\"\n",
    "res = answer_question(q, k=6)\n",
    "print(\"Answer:\\n\", res[\"answer\"])\n",
    "print(\"\\nSources:\", res[\"sources\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719dfdc8",
   "metadata": {},
   "source": [
    "## 12) Gradio Chatbot UI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a43ac2fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks(title=\"PDF RAG Chatbot\") as demo:\n",
    "    gr.Markdown(\"# üìö PDF RAG Chatbot ‚Äî Open‚ÄëSource LLM\")\n",
    "    gr.Markdown(\"Ask questions about your uploaded PDFs. Answers are grounded in the retrieved chunks and include citations.\")\n",
    "\n",
    "    chat = gr.Chatbot(height=400, type=\"messages\")\n",
    "    q_in = gr.Textbox(label=\"Your question\")\n",
    "    btn = gr.Button(\"Ask\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    def respond(history, query):\n",
    "        res = answer_question(query, k=6)\n",
    "        reply = res[\"answer\"]\n",
    "        # Add clickable sources list\n",
    "        if res[\"sources\"]:\n",
    "            reply += \"\\n\\n**Sources:** \" + \", \".join([f\"[{i+1}] {s}\" for i, s in enumerate(res[\"sources\"])])\n",
    "        history = history + [(query, reply)]\n",
    "        return history, \"\"\n",
    "\n",
    "    btn.click(respond, [chat, q_in], [chat, q_in])\n",
    "    clear.click(lambda: ([], \"\"), None, [chat, q_in])\n",
    "\n",
    "demo.launch(share=False)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
