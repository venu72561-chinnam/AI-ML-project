# -*- coding: utf-8 -*-
"""Mistral MPT multi model chatbot.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OKCVpf4MQ7Ghd1mxBNDljWJc_fWzzTmc
"""

!pip install -q transformers accelerate sentencepiece

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

model_id = "mistralai/Mistral-7B-Instruct-v0.2"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.float16,
    device_map="auto"
)

def chat_mistral(prompt):
    formatted_prompt = f"[INST] {prompt} [/INST]"
    inputs = tokenizer(formatted_prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=400)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

print(chat_mistral("what is AI"))

import os

offload_dir = "/content/mpt_offload"
os.makedirs(offload_dir, exist_ok=True)

model_id = "mosaicml/mpt-7b-instruct"

tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,      # use bfloat16 for Colab GPU
    device_map="auto",               # auto spread between GPU and CPU
    offload_folder=offload_dir,      # <â€” key fix
)

def chat_mpt(prompt):
    input_text = f"### Instruction:\n{prompt}\n### Response:\n"
    inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=400)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

print(chat_mpt("Explain supervised vs unsupervised learning in simple terms."))

import gradio as gr

def gradio_chat(prompt, model_choice):
    if model_choice == "Mistral":
        return chat_mistral(prompt)
    else:
        return chat_mpt(prompt)

gr.Interface(
    fn=gradio_chat,
    inputs=["text", gr.Radio(["Mistral", "MPT"], label="Choose Model")],
    outputs="text",
    title="Chatbot"
).launch()